{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gensim' has no attribute 'downloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48b13bf3b4d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#model = Word2Vec.load('pt.bin') ### carregando modelo pré-treinado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wiki_vectors_format_without_stopwords.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### normalizando modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gensim' has no attribute 'downloader'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyemd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer as rpt\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk import tokenize    \n",
    "from string import punctuation \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = sw.words('portuguese')\n",
    "    \n",
    "def parse(text):\n",
    "    words = []\n",
    "    word_pattern = rpt(r'\\w+')\n",
    "    \n",
    "    patterns = [word_pattern]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        tokens = []\n",
    "        for token in pattern.tokenize(text):\n",
    "            if token not in stopwords and len(token) > 3:\n",
    "                tokens.append(token.lower())\n",
    "        words.extend(tokens)\n",
    "    return words\n",
    "\n",
    "### carregando modelo pré-treinado e nomalizando-o\n",
    "model = KeyedVectors.load_word2vec_format(\"wiki_vectors_format_without_stopwords.bin\", binary=False)\n",
    "model.init_sims(replace=True) \n",
    "\n",
    "def headline_embeddings(headline):\n",
    "    emb = []\n",
    "    for word in headline:\n",
    "        emb.append(model[word])\n",
    "    return emb\n",
    "        \n",
    "headline = \"O que Bolsonaro e Moro colocam em jogo\"\n",
    "headline = parse(headline)\n",
    "hl_e1 = headline_embeddings(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headline_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aa5b7586ac10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mheadline2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Os prejuizos da Operacao Lava Jato para o Brasil\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mheadline2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhl_e2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheadline_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadline2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headline_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "headline2 = \"Os prejuizos da Operacao Lava Jato para o Brasil\"\n",
    "headline2 = parse(headline2)\n",
    "hl_e2 = headline_embeddings(headline2)\n",
    "\n",
    "from math import inf\n",
    "from math import sqrt\n",
    "\n",
    "def calc_wmd(vector1, vector2):\n",
    "    wmd = 0     \n",
    "    \n",
    "    assert len(vector1) == len(vector2)\n",
    "    \n",
    "    for w in vector1:\n",
    "        min_dist = inf\n",
    "        for w2 in vector2:\n",
    "            min_dist = min(min_dist, sqrt(sum((w - w2) ** 2)))\n",
    "        \n",
    "        wmd += min_dist * 1.0/len(vector1) ### normalize score\n",
    "        \n",
    "    return wmd\n",
    "\n",
    "calc_wmd(hl_e1, hl_e2)\n",
    "model.wmdistance(headline, headline2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Selecao brasileira joga proxima terca-feira\",\n",
    "        \"Neymar é acusado de estupro\",\n",
    "        \"Mitos e verdades sobre o consumo de pimenta para a saude\",\n",
    "        \"Saiba qual é o tecido que os gatos mais gostam de arranhar\",\n",
    "        \"Lava Jato desconfiava de Leo Pinheiro ate versao do triplex do Guaruja\",\n",
    "        \"China e EUA concordam em retomar negociacoes comerciais\",\n",
    "        \"As gambiarras que todo jogador ja fez na vida\",\n",
    "        \"O seda em fim de ciclo com vendas absurdas\",\n",
    "        \"Os prejuizos da Operacao Lava Jato para o Brasil\",\n",
    "        \"O que Bolsonaro e Moro colocam em jogo\"]\n",
    "\n",
    "processed_docs = [parse(w) for w in docs]\n",
    "\n",
    "from gensim.similarities import WmdSimilarity\n",
    "num_best = 3\n",
    "instance = WmdSimilarity(processed_docs, model, num_best)\n",
    "\n",
    "query =  parse(\"Bolsonaro e Trump reforcam lacos diplomaticos\")\n",
    "sim = instance[query]\n",
    "\n",
    "for i in range(num_best):\n",
    "    print ('Similaridade = %.4f' % sim[i][1])\n",
    "    print (docs[sim[i][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referencias \n",
    "[1]https://docs.google.com/presentation/d/1CTmpRHA2gurNM6APTpXzmpfVSZLgRoOszA3XQ-PyCto/edit#slide=id.gc6f73a04f_0_0\n",
    "[2]https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
